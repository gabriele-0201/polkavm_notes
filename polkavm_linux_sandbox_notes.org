* polkavm_linux_sandbox

** SandboxProgramInit

struct that stores some initial information for the SandboxProgram

#+begin_src rust
pub struct SandboxProgramInit<'a> {
    // This struct contain some blocks of the code
    // + ro_data (reference to the slice in memory)
    // + rw_data (reference to the slice in memory)
    // + bss_size
    // + stack_size
    guest_init: GuestProgramInit<'a>,
    code: &'a [u8],
    jump_table: &'a [u8],
    sysreturn_address: u64,
}
#+end_src
#+begin_src rust
pub struct SandboxProgram {
    memfd: Fd,
    memory_config: SandboxMemoryConfig,
    sysreturn_address: u64,
}
#+end_src

** SandboxMemoryConfig

present in zygote.rs

#+begin_src rust
pub struct SandboxMemoryConfig {
    // this field contain the referecent to
    // ro_data, rw_data, bss_size and stack_size
    guest_config: GuestMemoryConfig,
    code_size: u32,
    jump_table_size: u32,
}
#+end_src

** SandboxProgram

#+begin_src rust
pub struct SandboxProgram {
    // owned file descriptor, implemented in polkavm_linux_raw
    memfd: Fd,
    // Information about memory
    memory_config: SandboxMemoryConfig,
    sysreturn_address: u64,
}
#+end_src

*** new(SandboxProgramInit)

1. get the native_page_size
2. using the SandboxProgramInit argument the SandboxMemoryConfig is created
3. Call to `prepare_sealed_memfd` where the arguments are:
   + name of the fd
   + total size (ro_data + rw_data + code + jump_table)
   + a closure that append all the just linked segments into a buffer
4. that's it, just memory config + write the polkavm_blob into ad fd, wrap those things and return the SandboxProgram object


** get_native_page_size

REALLY interesting

but the function is liked at runtime? YUP, glibc is linked at runtime by the dynamic linker (ld in linux), all the elf files that uses dynamic libraries

#+begin_src rust
fn get_native_page_size() -> usize {
    // This is literally the only thing we need from `libc`, so instead of including
    // the whole crate let's just define these ourselves.

    const _SC_PAGESIZE: c_int = 30;
    extern "C" {
        fn sysconf(name: c_int) -> c_long;
    }

    unsafe { sysconf(_SC_PAGESIZE) as usize }
}
#+end_src

** prepare_sealed_memfd(name: &core::ffi::CStr, length: usize, populate: impl FnOnce(&mut [u8])

1. create a new fd
2. truncte to the correct size
3. create an MMAP object and attach an the fd
4. populate the fd using the passed closure
5. unmap the map
6. check on the unmap results, there are weird behaviours in multithreading

** Sandbox

#+begin_src rust
pub struct Sandbox {
    vmctx_mmap: Mmap,
    child: ChildProcess,
    socket: Fd,

    count_wait_loop_start: u64,
    count_futex_wait: u64,
}
#+end_src

*** Spawn
1. `Sigmask::block_all_signals()?`
2. `prepare_zygote()?`
3. `prepare_vmctx()?`
4. `let (socket, child_socket) = linux_raw::sys_socketpair(linux_raw::AF_UNIX, linux_raw::SOCK_SEQPACKET, 0)?`
   This syscall create a pair con CONNECTED socket (typically used for inter process communication)

   AF_UNIX => address family, the UNIX domain socket will be used
   SOCK_SEQPACKET => using `SOCK_SEQPACKET` sockets, you can send and receive complete packets (messages) of data, rather than dealing with a stream of data. provides a reliable and sequenced data transmission mechanism (data sent from one end will be received in the same order at the other end)
5. Setup sandbox_flags what will be used in the creation of the new process (the sandbox), using the **clonse** syscall, flags are:
   - CLONE_NEWCGROUP :: Creates a new control group namespace.
   - CLONE_NEWIPC :: Creates a new Inter-Process Communication (IPC) namespace.
   - CLONE_NEWNET :: Creates a new network namespace.
   - CLONE_NEWNS :: Creates a new mount namespace.
   - CLONE_NEWPID :: Creates a new PID namespace.
   - CLONE_NEWUSER :: Creates a new user namespace.
   - CLONE_NEWUTS :: Creates a new Unix Timesharing System (UTS) namespace.
   - CLONE_CLEAR_SIGHAND :: Clears the signal handlers in the child process.
   - CLONE_PIDFD :: Creates a file descriptor for the child's PID.

     There's NO CLONE_VM flags, this meant that there's NO shared virtual memory -> it can be descrbed as a project

     A namespace is a feature that provides process isolation by creating separate instances of global system resources for different processes. Each process can have its own unique view of certain resources, making it appear as if it is running in its own isolated environment
6. Setup the struct CloneArgs
7. Fetch the UserID and GroupID (UID and GID)
8. if logger enable then use `sys_pipe2` to create a connection to debug the code (TODO: is this correct?)
9. call `clone` -> get the child process id

   Now there is a split
    pid == 0 => child

    // NOT sure about this, maybe it is a way to FORGET
    // TODO WHY?
    core::mem::forget(sigset);

    call to child_main function
    
    pid != 0 => parent

10. SET UP SOME LOGGING STUFF IF ENABLED (TODO)
11. Construct ChildProcess struct
12. Close child_socket and unblock sigset
13. Prepare the VMCTX (TODO ...)
14.




** Sigmask

#+begin_src rust
struct Sigmask {
    sigset_original: linux_raw::kernel_sigset_t,
}
#+end_src

*** block_all_signals

Temporarily blocks all signals from being delivered.

#+begin_src rust
// kernel_sigset_t = core::ffi::c_ulong (usigned long)
let sigset_all: linux_raw::kernel_sigset_t = !0;
let mut sigset_original: linux_raw::kernel_sigset_t = 0;
unsafe { linux_raw::sys_rt_sigprocmask(linux_raw::SIG_SETMASK, &sigset_all, Some(&mut sigset_original))? };

Ok(Sigmask { sigset_original })
#+end_src

What `sigprocmask` does?

Each process can receive some signal from the kernel (`SIGTERM` (termination signal), `SIGINT` (interrupt signal), `SIGSEGV` (segmentation fault), `SIGKILL` (kill signal), and `SIGUSR1` (user-defined signal))

And the sigprocmask let you define a mask to BLOCK some signals, preventing them to be delivered to the process

** prepare_zygote

just load the bytecode of the zygote into a mmap (under a fd) using the function prepare_sealed_memfd

** prepare_vmctx


1. align the size of the vmctx to a multiple of the native_page_size
2. create memfd -> using MFD_CLOEXEC | MFD_ALLOW_SEALING flag (TODO: what those flag do?)
3. ftruncate the memfd to the vmctx size
4. call to fcntl (syscall that enable you to make various operation to the file)
   F_ADD_SEALS spcifie that new seals will be added, a seal is a restriction placed on an open file
   + F_SEAL_SEAL -> Prevent adding more seals
   + F_SEAL_SHRINK  -> prevent the file size reduction
   + F_SEAL_GROW -> prevent the file size growth
5. Create a Mmap object, it is just an abstraction over a memory mapped fd (the constructor just calls sys_mmap)
6. cast the mmap object and save into a new VmCtx object

** CloneArgs

#+begin_src rust
struct CloneArgs {
    /// Flags.
    flags: u64,
    /// Where to store PID file descriptor. (int *)
    pidfd: *mut c_int,
    /// Where to store child TID in child's memory. (pid_t *)
    child_tid: u64,
    /// Where to store child TID in parent's memory. (pid_t *)
    parent_tid: u64,
    /// Signal to deliver to parent on child termination.
    exit_signal: u64,
    /// Pointer to lowest byte of stack.
    stack: u64,
    /// Size of the stack.
    stack_size: u64,
    /// Location of the new TLS.
    tls: u64,
}
#+end_src
** Clone Syscall

Every process under Linux is dynamically allocated a struct task_struct structure

A process is a task that does not share virtual memory with its parent, whereas a thread is a task that shares virtual memory with its parent. It depends on which flags are passed to the clone syscall
** child_main

WWOOAAAHHH

(TODO: UNDESTAND EVERYTHIN)

stuff on groups, file descriptor and stuff

hide host filesystem
Clear all ambient capabilities.
Flag ourselves that we won't ever want to acquire any new privileges
Set resource limits (Virtual memory(?), stack and other thigs)

(TODO: WHAT?)
// Finally, drop all capabilities.
linux_raw::sys_capset_drop_all()?;

use sys_execveat to execute the zygote (throught the filedescriptor)
